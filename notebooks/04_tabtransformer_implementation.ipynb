{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2-1: TabTransformer êµ¬í˜„ (lucidrains/tab-transformer-pytorch ê¸°ë°˜)\n",
    "\n",
    "ê²€ì¦ëœ TabTransformer êµ¬í˜„ì²´ë¥¼ í™œìš©í•˜ì—¬ ì•” ì˜ˆí›„ ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„\n",
    "\n",
    "## ëª©í‘œ\n",
    "- **CoxTabTransformer**: Clinical categorical + Multi-omics [measured_value, cox_coefficient] pairs\n",
    "- **MethylationTabTransformer**: High-dimensional methylation data with learnable feature selection\n",
    "- **End Point**: 3ë…„ ìƒì¡´ ì—¬ë¶€ Binary Classification (0: ìƒì¡´, 1: 3ë…„ ë‚´ ì‚¬ë§)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "!pip install tab-transformer-pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append('../')\n",
    "from src.models.cox_tabtransformer import CoxTabTransformer\n",
    "from src.models.methylation_tabtransformer import MethylationTabTransformer\n",
    "from src.utils.tabtransformer_utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ì¡´ TabTransformer ë™ì‘ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ê¸°ì¡´ TabTransformer ë™ì‘ í™•ì¸ ===\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ì˜ˆì œë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "test_model = TabTransformer(\n",
    "    categories = (10, 5, 6),     # 3ê°œ ë²”ì£¼í˜• íŠ¹ì„±\n",
    "    num_continuous = 8,          # 8ê°œ ìˆ˜ì¹˜í˜• íŠ¹ì„±\n",
    "    dim = 32,\n",
    "    dim_out = 1,\n",
    "    depth = 3,\n",
    "    heads = 4\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "batch_size = 16\n",
    "categorical_test = torch.tensor([\n",
    "    [0, 1, 2],\n",
    "    [1, 2, 3],\n",
    "    [2, 3, 4],\n",
    "    [3, 4, 5]\n",
    "]).repeat(4, 1)  # 16 samples\n",
    "\n",
    "continuous_test = torch.randn(batch_size, 8)\n",
    "\n",
    "# Forward pass í…ŒìŠ¤íŠ¸\n",
    "output = test_model(categorical_test, continuous_test)\n",
    "\n",
    "print(f\"âœ… TabTransformer í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"   ë²”ì£¼í˜• ì…ë ¥ shape: {categorical_test.shape}\")\n",
    "print(f\"   ìˆ˜ì¹˜í˜• ì…ë ¥ shape: {continuous_test.shape}\")\n",
    "print(f\"   ì¶œë ¥ shape: {output.shape}\")\n",
    "print(f\"   ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in test_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "DATA_DIR = Path('../data/processed')\n",
    "RESULTS_DIR = Path('../results')\n",
    "MODELS_DIR = Path('../src/models')\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# Cox ê´€ë ¨ ë°ì´í„°\n",
    "cox_data = pd.read_parquet(DATA_DIR / 'integrated_table_cox.parquet')\n",
    "clinical_data = pd.read_parquet(DATA_DIR / 'processed_clinical_data.parquet')\n",
    "\n",
    "# Methylation ê´€ë ¨ ë°ì´í„°\n",
    "methylation_data = pd.read_parquet(DATA_DIR / 'methylation_table.parquet')\n",
    "clinical_data_methylation = pd.read_parquet(DATA_DIR / 'processed_clinical_data_for_methylation.parquet')\n",
    "\n",
    "print(f\"âœ… Cox ë°ì´í„° ë¡œë“œ: {cox_data.shape}\")\n",
    "print(f\"âœ… ì„ìƒ ë°ì´í„° (Cox): {clinical_data.shape}\")\n",
    "print(f\"âœ… ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„°: {methylation_data.shape}\")\n",
    "print(f\"âœ… ì„ìƒ ë°ì´í„° (Methylation): {clinical_data_methylation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cox ê³„ìˆ˜ ë¡œë“œ ë° ìƒì¡´ ë¼ë²¨ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cox ê³„ìˆ˜ ë¡œë“œ\n",
    "try:\n",
    "    cox_coefficients = load_cox_coefficients(DATA_DIR / 'cox_coefficients_lookup.pkl')\n",
    "    print(f\"âœ… Cox ê³„ìˆ˜ ë¡œë“œ ì™„ë£Œ: {len(cox_coefficients)}ê°œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Cox ê³„ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ìƒ˜í”Œ Cox ê³„ìˆ˜ ìƒì„±...\")\n",
    "    # ìƒ˜í”Œ Cox ê³„ìˆ˜ ìƒì„± (ì‹¤ì œë¡œëŠ” Cox regressionìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•¨)\n",
    "    cox_coefficients = {col: np.random.normal(0, 0.5) for col in cox_data.columns[:1000]}\n",
    "\n",
    "# Cox ë°ì´í„°ìš© ìƒì¡´ ë¼ë²¨ ìƒì„±\n",
    "survival_labels_cox, valid_patient_ids_cox = create_survival_labels(clinical_data, 1095)\n",
    "\n",
    "print(f\"\\n=== Cox ë°ì´í„° 3ë…„ ìƒì¡´ ì˜ˆì¸¡ End Point ===\")\n",
    "print(f\"ì „ì²´ í™˜ì ìˆ˜: {len(clinical_data)}\")\n",
    "print(f\"ìœ íš¨í•œ í™˜ì ìˆ˜: {len(survival_labels_cox)}\")\n",
    "print(f\"3ë…„ ìƒì¡´: {np.sum(survival_labels_cox == 0)} ({np.mean(survival_labels_cox == 0)*100:.1f}%)\")\n",
    "print(f\"3ë…„ ë‚´ ì‚¬ë§: {np.sum(survival_labels_cox == 1)} ({np.mean(survival_labels_cox == 1)*100:.1f}%)\")\n",
    "\n",
    "# Methylation ë°ì´í„°ìš© ìƒì¡´ ë¼ë²¨ ìƒì„±\n",
    "survival_labels_meth, valid_patient_ids_meth = create_survival_labels(clinical_data_methylation, 1095)\n",
    "\n",
    "print(f\"\\n=== Methylation ë°ì´í„° 3ë…„ ìƒì¡´ ì˜ˆì¸¡ End Point ===\")\n",
    "print(f\"ì „ì²´ í™˜ì ìˆ˜: {len(clinical_data_methylation)}\")\n",
    "print(f\"ìœ íš¨í•œ í™˜ì ìˆ˜: {len(survival_labels_meth)}\")\n",
    "print(f\"3ë…„ ìƒì¡´: {np.sum(survival_labels_meth == 0)} ({np.mean(survival_labels_meth == 0)*100:.1f}%)\")\n",
    "print(f\"3ë…„ ë‚´ ì‚¬ë§: {np.sum(survival_labels_meth == 1)} ({np.mean(survival_labels_meth == 1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cox ë°ì´í„° ì „ì²˜ë¦¬ (Clinical + Multi-omics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cox ë°ì´í„° ì „ì²˜ë¦¬ ===\")\n",
    "\n",
    "# ìœ íš¨í•œ í™˜ìë“¤ë¡œ ë°ì´í„° í•„í„°ë§\n",
    "cox_data_filtered = cox_data.loc[cox_data.index.intersection(valid_patient_ids_cox)]\n",
    "clinical_data_filtered = clinical_data.loc[clinical_data.index.intersection(valid_patient_ids_cox)]\n",
    "\n",
    "print(f\"í•„í„°ë§ëœ Cox ë°ì´í„°: {cox_data_filtered.shape}\")\n",
    "print(f\"í•„í„°ë§ëœ ì„ìƒ ë°ì´í„°: {clinical_data_filtered.shape}\")\n",
    "\n",
    "# Cox [ì¸¡ì •ê°’, ê³„ìˆ˜] ìŒ ë°ì´í„° ì¤€ë¹„\n",
    "cox_continuous, cox_feature_names = prepare_cox_data(\n",
    "    cox_data_filtered, \n",
    "    cox_coefficients\n",
    ")\n",
    "\n",
    "# ì„ìƒ ë²”ì£¼í˜• ë°ì´í„° ì¤€ë¹„\n",
    "clinical_categorical, vocab_sizes, encoders, clinical_feature_names = prepare_clinical_data(\n",
    "    clinical_data_filtered\n",
    ")\n",
    "\n",
    "print(f\"\\nCox continuous shape: {cox_continuous.shape}\")\n",
    "print(f\"Clinical categorical shape: {clinical_categorical.shape}\")\n",
    "print(f\"Vocabulary sizes: {vocab_sizes}\")\n",
    "print(f\"Clinical features: {clinical_feature_names}\")\n",
    "\n",
    "# ê³µí†µ í™˜ì ì¸ë±ìŠ¤ë¡œ ì •ë ¬\n",
    "common_cox_patients = cox_data_filtered.index.tolist()\n",
    "labels_dict_cox = dict(zip(valid_patient_ids_cox, survival_labels_cox))\n",
    "cox_labels_aligned = np.array([labels_dict_cox[pid] for pid in common_cox_patients])\n",
    "\n",
    "print(f\"ì •ë ¬ëœ Cox ë¼ë²¨ shape: {cox_labels_aligned.shape}\")\n",
    "print(f\"Cox ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CoxTabTransformer ëª¨ë¸ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CoxTabTransformer ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "cox_model_config = {\n",
    "    'clinical_categories': vocab_sizes,\n",
    "    'num_omics_features': len(cox_feature_names),\n",
    "    'dim': 64,\n",
    "    'depth': 4,\n",
    "    'heads': 8,\n",
    "    'attn_dropout': 0.1,\n",
    "    'ff_dropout': 0.1,\n",
    "    'survival_hidden_dim': 256\n",
    "}\n",
    "\n",
    "print(f\"ëª¨ë¸ ì„¤ì •: {cox_model_config}\")\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "cox_model = CoxTabTransformer(**cox_model_config)\n",
    "cox_model = cox_model.to(device)\n",
    "\n",
    "print(f\"CoxTabTransformer íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in cox_model.parameters()):,}\")\n",
    "\n",
    "# Forward pass í…ŒìŠ¤íŠ¸\n",
    "with torch.no_grad():\n",
    "    # ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸\n",
    "    test_clinical = clinical_categorical[:8].to(device)\n",
    "    test_omics = cox_continuous[:8].to(device)\n",
    "    \n",
    "    survival_logit, representation = cox_model(test_clinical, test_omics)\n",
    "    \n",
    "    print(f\"âœ… CoxTabTransformer forward pass ì„±ê³µ\")\n",
    "    print(f\"   ì„ìƒ ì…ë ¥ shape: {test_clinical.shape}\")\n",
    "    print(f\"   ì˜¤ë¯¹ìŠ¤ ì…ë ¥ shape: {test_omics.shape}\")\n",
    "    print(f\"   ìƒì¡´ logit shape: {survival_logit.shape}\")\n",
    "    print(f\"   Representation shape: {representation.shape}\")\n",
    "    \n",
    "    # Probability ë³€í™˜\n",
    "    survival_prob = torch.sigmoid(survival_logit)\n",
    "    print(f\"   ìƒì¡´ í™•ë¥  ë²”ìœ„: {survival_prob.min().item():.3f} - {survival_prob.max().item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ì „ì²˜ë¦¬ ===\")\n",
    "\n",
    "# ìœ íš¨í•œ í™˜ìë“¤ë¡œ í•„í„°ë§\n",
    "methylation_filtered = methylation_data.loc[\n",
    "    methylation_data.index.intersection(valid_patient_ids_meth)\n",
    "]\n",
    "\n",
    "print(f\"í•„í„°ë§ëœ ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„°: {methylation_filtered.shape}\")\n",
    "\n",
    "# ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ì „ì²˜ë¦¬ (variance filtering)\n",
    "methylation_tensor, selected_probe_names = prepare_methylation_data(\n",
    "    methylation_filtered, \n",
    "    variance_threshold=0.01\n",
    ")\n",
    "\n",
    "# ë¼ë²¨ ì •ë ¬\n",
    "common_meth_patients = methylation_filtered.index.tolist()\n",
    "labels_dict_meth = dict(zip(valid_patient_ids_meth, survival_labels_meth))\n",
    "meth_labels_aligned = np.array([labels_dict_meth[pid] for pid in common_meth_patients])\n",
    "\n",
    "print(f\"ì „ì²˜ë¦¬ëœ ë©”í‹¸ë ˆì´ì…˜ tensor shape: {methylation_tensor.shape}\")\n",
    "print(f\"ì„ íƒëœ probe ìˆ˜: {len(selected_probe_names)}\")\n",
    "print(f\"ë©”í‹¸ë ˆì´ì…˜ ë¼ë²¨ shape: {meth_labels_aligned.shape}\")\n",
    "print(f\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆìƒ: {methylation_tensor.numel() * 4 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MethylationTabTransformer ëª¨ë¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MethylationTabTransformer ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "meth_model_config = {\n",
    "    'num_probes': methylation_tensor.shape[1],\n",
    "    'selected_probes': 2000,  # ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ì¡°ì •\n",
    "    'dim': 64,\n",
    "    'depth': 3,\n",
    "    'heads': 8,\n",
    "    'attn_dropout': 0.1,\n",
    "    'ff_dropout': 0.1,\n",
    "    'survival_hidden_dim': 256\n",
    "}\n",
    "\n",
    "print(f\"ëª¨ë¸ ì„¤ì •: {meth_model_config}\")\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "meth_model = MethylationTabTransformer(**meth_model_config)\n",
    "meth_model = meth_model.to(device)\n",
    "\n",
    "print(f\"MethylationTabTransformer íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in meth_model.parameters()):,}\")\n",
    "\n",
    "# Forward pass í…ŒìŠ¤íŠ¸\n",
    "with torch.no_grad():\n",
    "    # ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸\n",
    "    test_meth = methylation_tensor[:4].to(device)\n",
    "    \n",
    "    survival_logit, representation, selected_indices = meth_model(test_meth)\n",
    "    \n",
    "    print(f\"âœ… MethylationTabTransformer forward pass ì„±ê³µ\")\n",
    "    print(f\"   ë©”í‹¸ë ˆì´ì…˜ ì…ë ¥ shape: {test_meth.shape}\")\n",
    "    print(f\"   ìƒì¡´ logit shape: {survival_logit.shape}\")\n",
    "    print(f\"   Representation shape: {representation.shape}\")\n",
    "    print(f\"   ì„ íƒëœ ì¸ë±ìŠ¤ shape: {selected_indices.shape}\")\n",
    "    \n",
    "    # ì„ íƒëœ probe ì •ë³´\n",
    "    print(f\"   ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œ ì„ íƒëœ probe ì˜ˆì‹œ: {selected_indices[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë°ì´í„° ë¶„í•  ë° DataLoader ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ë°ì´í„° ë¶„í•  ===\")\n",
    "\n",
    "# Cox ë°ì´í„° ë¶„í• \n",
    "if len(cox_labels_aligned) > 100:  # ìµœì†Œ ë°ì´í„° ìˆ˜ í™•ì¸\n",
    "    # Clinicalê³¼ Omics ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶„í• \n",
    "    combined_cox_data = torch.cat([clinical_categorical, cox_continuous], dim=1)\n",
    "    \n",
    "    X_train_cox, X_val_cox, X_test_cox, y_train_cox, y_val_cox, y_test_cox = split_data_stratified(\n",
    "        combined_cox_data, cox_labels_aligned, \n",
    "        test_size=0.2, val_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Clinicalê³¼ Omics ë¶€ë¶„ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬\n",
    "    clinical_dim = clinical_categorical.shape[1]\n",
    "    \n",
    "    X_train_clinical = X_train_cox[:, :clinical_dim]\n",
    "    X_train_omics = X_train_cox[:, clinical_dim:]\n",
    "    X_val_clinical = X_val_cox[:, :clinical_dim]\n",
    "    X_val_omics = X_val_cox[:, clinical_dim:]\n",
    "    X_test_clinical = X_test_cox[:, :clinical_dim]\n",
    "    X_test_omics = X_test_cox[:, clinical_dim:]\n",
    "    \n",
    "    print(f\"Cox ë°ì´í„° ë¶„í• :\")\n",
    "    print(f\"  Train: {len(y_train_cox)} samples\")\n",
    "    print(f\"  Val: {len(y_val_cox)} samples\")\n",
    "    print(f\"  Test: {len(y_test_cox)} samples\")\n",
    "    \n",
    "    # DataLoader ìƒì„± (Cox)\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "    train_dataset_cox = TensorDataset(\n",
    "        X_train_clinical.long(), X_train_omics, \n",
    "        torch.tensor(y_train_cox, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset_cox = TensorDataset(\n",
    "        X_val_clinical.long(), X_val_omics, \n",
    "        torch.tensor(y_val_cox, dtype=torch.float32)\n",
    "    )\n",
    "    test_dataset_cox = TensorDataset(\n",
    "        X_test_clinical.long(), X_test_omics, \n",
    "        torch.tensor(y_test_cox, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_loader_cox = DataLoader(train_dataset_cox, batch_size=32, shuffle=True)\n",
    "    val_loader_cox = DataLoader(val_dataset_cox, batch_size=32, shuffle=False)\n",
    "    test_loader_cox = DataLoader(test_dataset_cox, batch_size=32, shuffle=False)\n",
    "    \n",
    "    cox_data_ready = True\n",
    "else:\n",
    "    print(\"Cox ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
    "    cox_data_ready = False\n",
    "\n",
    "# ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ë¶„í• \n",
    "if len(meth_labels_aligned) > 100:\n",
    "    X_train_meth, X_val_meth, X_test_meth, y_train_meth, y_val_meth, y_test_meth = split_data_stratified(\n",
    "        methylation_tensor, meth_labels_aligned,\n",
    "        test_size=0.2, val_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\në©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ë¶„í• :\")\n",
    "    print(f\"  Train: {len(y_train_meth)} samples\")\n",
    "    print(f\"  Val: {len(y_val_meth)} samples\")\n",
    "    print(f\"  Test: {len(y_test_meth)} samples\")\n",
    "    \n",
    "    # DataLoader ìƒì„± (Methylation)\n",
    "    train_dataset_meth = TensorDataset(X_train_meth, torch.tensor(y_train_meth, dtype=torch.float32))\n",
    "    val_dataset_meth = TensorDataset(X_val_meth, torch.tensor(y_val_meth, dtype=torch.float32))\n",
    "    test_dataset_meth = TensorDataset(X_test_meth, torch.tensor(y_test_meth, dtype=torch.float32))\n",
    "    \n",
    "    train_loader_meth = DataLoader(train_dataset_meth, batch_size=16, shuffle=True)  # ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    val_loader_meth = DataLoader(val_dataset_meth, batch_size=16, shuffle=False)\n",
    "    test_loader_meth = DataLoader(test_dataset_meth, batch_size=16, shuffle=False)\n",
    "    \n",
    "    meth_data_ready = True\n",
    "else:\n",
    "    print(\"ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n",
    "    meth_data_ready = False\n",
    "\n",
    "print(\"\\nâœ… ë°ì´í„° ë¶„í•  ë° DataLoader ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í›ˆë ¨ í•¨ìˆ˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cox_tabtransformer(model, train_loader, val_loader, epochs=20, lr=1e-4):\n",
    "    \"\"\"\n",
    "    CoxTabTransformer í›ˆë ¨ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.7)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for clinical_batch, omics_batch, labels_batch in progress_bar:\n",
    "            clinical_batch = clinical_batch.to(device)\n",
    "            omics_batch = omics_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(clinical_batch, omics_batch)\n",
    "            loss = criterion(logits.squeeze(), labels_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item():.4f})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for clinical_batch, omics_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                clinical_batch = clinical_batch.to(device)\n",
    "                omics_batch = omics_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                \n",
    "                logits, _ = model(clinical_batch, omics_batch)\n",
    "                loss = criterion(logits.squeeze(), labels_batch)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_labels_list.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Metrics ê³„ì‚°\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        val_auc = roc_auc_score(val_labels_list, val_preds)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"         Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Best model ì €ì¥\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save(model.state_dict(), 'best_cox_tabtransformer.pth')\n",
    "            print(f\"  âœ… Best model saved (AUC: {val_auc:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc\n",
    "    }\n",
    "\n",
    "def train_methylation_tabtransformer(model, train_loader, val_loader, epochs=15, lr=5e-5):\n",
    "    \"\"\"\n",
    "    MethylationTabTransformer í›ˆë ¨ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.7)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for meth_batch, labels_batch in progress_bar:\n",
    "            meth_batch = meth_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _, _ = model(meth_batch)\n",
    "            loss = criterion(logits.squeeze(), labels_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            train_labels.extend(labels_batch.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item():.4f})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for meth_batch, labels_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "                meth_batch = meth_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                \n",
    "                logits, _, _ = model(meth_batch)\n",
    "                loss = criterion(logits.squeeze(), labels_batch)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_labels_list.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Metrics ê³„ì‚°\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        val_auc = roc_auc_score(val_labels_list, val_preds)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"         Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Best model ì €ì¥\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save(model.state_dict(), 'best_methylation_tabtransformer.pth')\n",
    "            print(f\"  âœ… Best model saved (AUC: {val_auc:.4f})\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc\n",
    "    }\n",
    "\n",
    "print(\"âœ… í›ˆë ¨ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoxTabTransformer í›ˆë ¨\n",
    "if cox_data_ready:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CoxTabTransformer í›ˆë ¨ ì‹œì‘...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cox_history = train_cox_tabtransformer(\n",
    "        cox_model,\n",
    "        train_loader_cox,\n",
    "        val_loader_cox,\n",
    "        epochs=10,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì ì€ ì—í­\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ† CoxTabTransformer ìµœê³  ì„±ëŠ¥: {cox_history['best_val_auc']:.4f}\")\n",
    "else:\n",
    "    print(\"Cox ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MethylationTabTransformer í›ˆë ¨\n",
    "if meth_data_ready:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MethylationTabTransformer í›ˆë ¨ ì‹œì‘...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    meth_history = train_methylation_tabtransformer(\n",
    "        meth_model,\n",
    "        train_loader_meth,\n",
    "        val_loader_meth,\n",
    "        epochs=8,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì ì€ ì—í­\n",
    "        lr=5e-5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ† MethylationTabTransformer ìµœê³  ì„±ëŠ¥: {meth_history['best_val_auc']:.4f}\")\n",
    "else:\n",
    "    print(\"ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    \"\"\"í›ˆë ¨ ê³¼ì • ì‹œê°í™”\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss ê³¡ì„ \n",
    "    axes[0].plot(history['train_losses'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_losses'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{title} - Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC ê³¡ì„ \n",
    "    axes[1].plot(history['val_aucs'], label='Val AUC', marker='s', color='orange')\n",
    "    axes[1].axhline(y=history['best_val_auc'], color='r', linestyle='--', \n",
    "                   label=f'Best AUC: {history[\"best_val_auc\"]:.4f}')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('AUC')\n",
    "    axes[1].set_title(f'{title} - Validation AUC')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim([0.5, 1.0])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}_training.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "if cox_data_ready and 'cox_history' in locals():\n",
    "    plot_training_history(cox_history, 'CoxTabTransformer')\n",
    "\n",
    "if meth_data_ready and 'meth_history' in locals():\n",
    "    plot_training_history(meth_history, 'MethylationTabTransformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ëª¨ë¸ í‰ê°€ ë° í•´ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_name, is_cox_model=True):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            if is_cox_model:\n",
    "                clinical_batch, omics_batch, labels_batch = batch\n",
    "                clinical_batch = clinical_batch.to(device)\n",
    "                omics_batch = omics_batch.to(device)\n",
    "                logits, _ = model(clinical_batch, omics_batch)\n",
    "            else:\n",
    "                meth_batch, labels_batch = batch\n",
    "                meth_batch = meth_batch.to(device)\n",
    "                logits, _, _ = model(meth_batch)\n",
    "            \n",
    "            test_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            test_labels.extend(labels_batch.numpy())\n",
    "    \n",
    "    test_preds = np.array(test_preds).squeeze()\n",
    "    test_labels = np.array(test_labels).squeeze()\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    test_auc = roc_auc_score(test_labels, test_preds)\n",
    "    test_preds_binary = (test_preds > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(test_labels, test_preds_binary)\n",
    "    \n",
    "    print(f\"\\n=== {model_name} Test Performance ===\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
    "    print(f\"FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds_binary, \n",
    "                              target_names=['Survived (0)', 'Died (1)']))\n",
    "    \n",
    "    return test_auc, test_acc\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ ì‹¤í–‰\n",
    "if cox_data_ready and 'cox_history' in locals():\n",
    "    # Best model ë¡œë“œ\n",
    "    cox_model.load_state_dict(torch.load('best_cox_tabtransformer.pth'))\n",
    "    cox_test_auc, cox_test_acc = evaluate_model(\n",
    "        cox_model, test_loader_cox, \"CoxTabTransformer\", is_cox_model=True\n",
    "    )\n",
    "\n",
    "if meth_data_ready and 'meth_history' in locals():\n",
    "    # Best model ë¡œë“œ\n",
    "    meth_model.load_state_dict(torch.load('best_methylation_tabtransformer.pth'))\n",
    "    meth_test_auc, meth_test_acc = evaluate_model(\n",
    "        meth_model, test_loader_meth, \"MethylationTabTransformer\", is_cox_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Feature Importance ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”í‹¸ë ˆì´ì…˜ ëª¨ë¸ì˜ ì„ íƒëœ probe ë¶„ì„\n",
    "if meth_data_ready and 'meth_history' in locals():\n",
    "    print(\"=== ë©”í‹¸ë ˆì´ì…˜ Feature Importance ë¶„ì„ ===\")\n",
    "    \n",
    "    # ëª¨ë¸ì—ì„œ ì¤‘ìš”í•œ probe ì¶”ì¶œ\n",
    "    with torch.no_grad():\n",
    "        sample_meth = methylation_tensor[:10].to(device)\n",
    "        importance_scores, top_indices = meth_model.extract_feature_importance(sample_meth)\n",
    "        \n",
    "        print(f\"ìƒìœ„ 20ê°œ ì¤‘ìš”í•œ probe:\")\n",
    "        top_20_indices = top_indices[:20].cpu().numpy()\n",
    "        top_20_scores = importance_scores[top_20_indices].cpu().numpy()\n",
    "        \n",
    "        for i, (idx, score) in enumerate(zip(top_20_indices, top_20_scores)):\n",
    "            probe_name = selected_probe_names[idx] if idx < len(selected_probe_names) else f\"Probe_{idx}\"\n",
    "            print(f\"{i+1:2d}. {probe_name}: {score:.4f}\")\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(20), top_20_scores)\n",
    "    plt.xlabel('Top 20 Probes (ranked by importance)')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.title('Top 20 Most Important Methylation Probes')\n",
    "    plt.xticks(range(20), [f'P{i+1}' for i in range(20)], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('methylation_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ìµœì¢… ìš”ì•½ ë° ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "results_summary = {\n",
    "    'experiment_info': {\n",
    "        'framework': 'lucidrains/tab-transformer-pytorch',\n",
    "        'endpoint': '3-year survival binary classification',\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'data_info': {\n",
    "        'cox_patients': len(cox_labels_aligned) if cox_data_ready else 0,\n",
    "        'methylation_patients': len(meth_labels_aligned) if meth_data_ready else 0,\n",
    "        'clinical_features': len(clinical_feature_names) if cox_data_ready else 0,\n",
    "        'omics_features': len(cox_feature_names) if cox_data_ready else 0,\n",
    "        'methylation_probes': methylation_tensor.shape[1] if meth_data_ready else 0\n",
    "    },\n",
    "    'model_performance': {}\n",
    "}\n",
    "\n",
    "if cox_data_ready and 'cox_history' in locals():\n",
    "    results_summary['model_performance']['cox_tabtransformer'] = {\n",
    "        'best_val_auc': cox_history['best_val_auc'],\n",
    "        'test_auc': cox_test_auc if 'cox_test_auc' in locals() else None,\n",
    "        'test_accuracy': cox_test_acc if 'cox_test_acc' in locals() else None,\n",
    "        'parameters': sum(p.numel() for p in cox_model.parameters())\n",
    "    }\n",
    "\n",
    "if meth_data_ready and 'meth_history' in locals():\n",
    "    results_summary['model_performance']['methylation_tabtransformer'] = {\n",
    "        'best_val_auc': meth_history['best_val_auc'],\n",
    "        'test_auc': meth_test_auc if 'meth_test_auc' in locals() else None,\n",
    "        'test_accuracy': meth_test_acc if 'meth_test_acc' in locals() else None,\n",
    "        'parameters': sum(p.numel() for p in meth_model.parameters()),\n",
    "        'selected_probes': meth_model_config['selected_probes']\n",
    "    }\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "with open(RESULTS_DIR / 'tabtransformer_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(results_summary, indent=2))\n",
    "print(\"\\nâœ… TabTransformer êµ¬í˜„ ë° ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(f\"âœ… ê²°ê³¼ ì €ì¥ë¨: {RESULTS_DIR / 'tabtransformer_results.json'}\")\n",
    "\n",
    "if cox_data_ready and 'cox_history' in locals():\n",
    "    print(f\"ğŸ¯ CoxTabTransformer ìµœê³  ì„±ëŠ¥: {cox_history['best_val_auc']:.4f} AUC\")\n",
    "\n",
    "if meth_data_ready and 'meth_history' in locals():\n",
    "    print(f\"ğŸ¯ MethylationTabTransformer ìµœê³  ì„±ëŠ¥: {meth_history['best_val_auc']:.4f} AUC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}