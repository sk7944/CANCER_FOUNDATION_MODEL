import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report
from tqdm import tqdm
import json
import argparse
from pathlib import Path
import sys
import warnings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë™ì  íƒì§€
current_dir = Path(__file__).resolve().parent
project_root = current_dir
while project_root.name != 'CANCER_FOUNDATION_MODEL' and project_root.parent != project_root:
    project_root = project_root.parent
if project_root.name == 'CANCER_FOUNDATION_MODEL':
    sys.path.insert(0, str(project_root))
from src.models.cox_tabtransformer import CoxTabTransformer
from src.models.methylation_tabtransformer import MethylationTabTransformer
from src.utils.tabtransformer_utils import *

warnings.filterwarnings('ignore')

def train_cox_tabtransformer(model, train_loader, val_loader, epochs=100, lr=1e-4, device='cuda', 
                            checkpoint_dir=None, resume_from=None, target_auc=0.85):
    """
    CoxTabTransformer í›ˆë ¨ í•¨ìˆ˜
    
    Args:
        model: CoxTabTransformer ëª¨ë¸
        train_loader: í›ˆë ¨ DataLoader
        val_loader: ê²€ì¦ DataLoader
        epochs: í›ˆë ¨ ì—í­ ìˆ˜
        lr: í•™ìŠµë¥ 
        device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬
        resume_from: ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        target_auc: ëª©í‘œ AUC ì ìˆ˜
    
    Returns:
        history: í›ˆë ¨ ê¸°ë¡ ë”•ì…”ë„ˆë¦¬
    """
    model = model.to(device)
    
    # ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ ë³´ìˆ˜ì ì¸ ìµœì í™” ì „ëµ
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2, betas=(0.9, 0.999))  # ë” ê°•í•œ weight_decay
    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.2))  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤
    
    # ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ ë” ë¹ ë¥¸ í•™ìŠµë¥  ê°ì†Œ
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.3, patience=5, min_lr=1e-8  # ë” ë¹ ë¥¸ ê°ì†Œ, ë” ì§§ì€ patience
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    if checkpoint_dir:
        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
    
    # í›ˆë ¨ ì¬ê°œ
    start_epoch = 0
    best_val_auc = 0.0
    if resume_from and Path(resume_from).exists():
        print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ ì¬ê°œ: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_auc = checkpoint.get('best_val_auc', 0.0)
        print(f"ì—í­ {start_epoch}ë¶€í„° ì¬ê°œ, ìµœê³  ê²€ì¦ AUC: {best_val_auc:.4f}")
    
    # Early stopping ì„¤ì •
    patience_counter = 0
    early_stop_patience = 10  # 10 ì—í­ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
    
    history = {
        'train_losses': [],
        'val_losses': [],
        'train_aucs': [],
        'val_aucs': [],
        'train_accs': [],
        'val_accs': [],
        'best_val_auc': best_val_auc,
        'best_epoch': 0
    }
    
    for epoch in range(start_epoch, epochs):
        # Training
        model.train()
        train_loss = 0
        train_preds = []
        train_labels = []
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
        for clinical_batch, omics_batch, targets_batch in progress_bar:
            clinical_batch = clinical_batch.to(device)
            omics_batch = omics_batch.to(device)
            targets_batch = targets_batch.to(device)
            
            optimizer.zero_grad()
            
            # Forward pass
            logits, _ = model(clinical_batch, omics_batch)
            loss = criterion(logits.squeeze(), targets_batch)
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())
            train_labels.extend(targets_batch.cpu().numpy())
            
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # Validation
        model.eval()
        val_loss = 0
        val_preds = []
        val_labels = []
        
        with torch.no_grad():
            for clinical_batch, omics_batch, targets_batch in tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]", leave=False):
                clinical_batch = clinical_batch.to(device)
                omics_batch = omics_batch.to(device)
                targets_batch = targets_batch.to(device)
                
                logits, _ = model(clinical_batch, omics_batch)
                loss = criterion(logits.squeeze(), targets_batch)
                
                val_loss += loss.item()
                val_preds.extend(torch.sigmoid(logits).cpu().numpy())
                val_labels.extend(targets_batch.cpu().numpy())
        
        # Metrics ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        train_auc = roc_auc_score(train_labels, train_preds) if len(set(train_labels)) > 1 else 0
        val_auc = roc_auc_score(val_labels, val_preds) if len(set(val_labels)) > 1 else 0
        
        # Accuracy ê³„ì‚°
        train_preds_binary = [1 if p > 0.5 else 0 for p in train_preds]
        val_preds_binary = [1 if p > 0.5 else 0 for p in val_preds]
        train_acc = accuracy_score(train_labels, train_preds_binary)
        val_acc = accuracy_score(val_labels, val_preds_binary)
        
        # ê¸°ë¡ ì €ì¥
        history['train_losses'].append(train_loss)
        history['val_losses'].append(val_loss)
        history['train_aucs'].append(train_auc)
        history['val_aucs'].append(val_auc)
        if 'train_accs' not in history:
            history['train_accs'] = []
            history['val_accs'] = []
        history['train_accs'].append(train_acc)
        history['val_accs'].append(val_acc)
        
        print(f"Epoch {epoch+1:3d}: Loss(T/V): {train_loss:.3f}/{val_loss:.3f} | "
              f"AUC(T/V): {train_auc:.3f}/{val_auc:.3f} | "
              f"Acc(T/V): {train_acc:.3f}/{val_acc:.3f}")
        
        # Best model ì €ì¥ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if val_auc > history['best_val_auc']:
            history['best_val_auc'] = val_auc
            history['best_epoch'] = epoch + 1
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            best_model_path = str(checkpoint_dir / 'best_cox_tabtransformer.pth') if checkpoint_dir else 'best_cox_tabtransformer.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_auc': val_auc,
                'history': history
            }, best_model_path)
            print(f"  âœ… Best model saved (AUC: {val_auc:.4f}) at {best_model_path}")
            
            # ëª©í‘œ AUC ë‹¬ì„± ì•Œë¦¼ (í•˜ì§€ë§Œ ê³„ì† í›ˆë ¨)
            if val_auc >= target_auc:
                print(f"  ğŸ¯ Target AUC {target_auc:.3f} achieved! Continuing training for better performance...")
            
            # Early stopping ì¹´ìš´í„° ë¦¬ì…‹
            patience_counter = 0
        else:
            # ê°œì„ ë˜ì§€ ì•ŠìŒ - patience ì¦ê°€
            patience_counter += 1
        
        # Early stopping ì²´í¬
        if patience_counter >= early_stop_patience:
            print(f"\nâ¹ï¸  Early stopping triggered after {patience_counter} epochs without improvement")
            print(f"   Best Val AUC: {history['best_val_auc']:.4f} (Epoch {history['best_epoch']})")
            break
        
        # ë§¤ ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if checkpoint_dir and epoch % 5 == 0:  # 5 ì—í­ë§ˆë‹¤ ì €ì¥
            checkpoint_path = str(checkpoint_dir / f'cox_checkpoint_epoch_{epoch}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_auc': history['best_val_auc'],
                'history': history
            }, checkpoint_path)
            print(f"  ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        
        # Learning rate scheduling (val_auc ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½)
        scheduler.step(val_auc)
        
        # Early stopping (optional)
        if epoch - history['best_epoch'] > 10 and epoch > 20:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    return history

def train_methylation_tabtransformer(model, train_loader, val_loader, epochs=50, lr=5e-5, device='cuda',
                                   checkpoint_dir=None, resume_from=None, target_auc=0.85):
    """
    MethylationTabTransformer í›ˆë ¨ í•¨ìˆ˜
    
    Args:
        model: MethylationTabTransformer ëª¨ë¸
        train_loader: í›ˆë ¨ DataLoader
        val_loader: ê²€ì¦ DataLoader
        epochs: í›ˆë ¨ ì—í­ ìˆ˜
        lr: í•™ìŠµë¥ 
        device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
    
    Returns:
        history: í›ˆë ¨ ê¸°ë¡ ë”•ì…”ë„ˆë¦¬
    """
    model = model.to(device)
    
    # ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ ë³´ìˆ˜ì ì¸ ìµœì í™” ì „ëµ
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2, betas=(0.9, 0.999))  # ë” ê°•í•œ weight_decay
    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.2))  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤
    
    # ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ ë” ë¹ ë¥¸ í•™ìŠµë¥  ê°ì†Œ
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.3, patience=5, min_lr=1e-8  # ë” ë¹ ë¥¸ ê°ì†Œ, ë” ì§§ì€ patience
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    if checkpoint_dir:
        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
    
    # í›ˆë ¨ ì¬ê°œ
    start_epoch = 0
    best_val_auc = 0.0
    if resume_from and Path(resume_from).exists():
        print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ í›ˆë ¨ ì¬ê°œ: {resume_from}")
        checkpoint = torch.load(resume_from, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_auc = checkpoint.get('best_val_auc', 0.0)
        print(f"ì—í­ {start_epoch}ë¶€í„° ì¬ê°œ, ìµœê³  ê²€ì¦ AUC: {best_val_auc:.4f}")
    
    # Early stopping ì„¤ì •
    patience_counter = 0
    early_stop_patience = 10  # 10 ì—í­ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
    
    history = {
        'train_losses': [],
        'val_losses': [],
        'train_aucs': [],
        'val_aucs': [],
        'train_accs': [],
        'val_accs': [],
        'best_val_auc': best_val_auc,
        'best_epoch': 0
    }
    
    for epoch in range(start_epoch, epochs):
        # Training
        model.train()
        train_loss = 0
        train_preds = []
        train_labels = []
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
        for meth_batch, targets_batch in progress_bar:
            meth_batch = meth_batch.to(device)
            targets_batch = targets_batch.to(device)
            
            optimizer.zero_grad()
            
            # Forward pass
            logits, _, _ = model(meth_batch)
            loss = criterion(logits.squeeze(), targets_batch)
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())
            train_labels.extend(targets_batch.cpu().numpy())
            
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # Validation
        model.eval()
        val_loss = 0
        val_preds = []
        val_labels = []
        
        with torch.no_grad():
            for meth_batch, targets_batch in tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]", leave=False):
                meth_batch = meth_batch.to(device)
                targets_batch = targets_batch.to(device)
                
                logits, _, _ = model(meth_batch)
                loss = criterion(logits.squeeze(), targets_batch)
                
                val_loss += loss.item()
                val_preds.extend(torch.sigmoid(logits).cpu().numpy())
                val_labels.extend(targets_batch.cpu().numpy())
        
        # Metrics ê³„ì‚°
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        train_auc = roc_auc_score(train_labels, train_preds) if len(set(train_labels)) > 1 else 0
        val_auc = roc_auc_score(val_labels, val_preds) if len(set(val_labels)) > 1 else 0
        
        # Accuracy ê³„ì‚°
        train_preds_binary = [1 if p > 0.5 else 0 for p in train_preds]
        val_preds_binary = [1 if p > 0.5 else 0 for p in val_preds]
        train_acc = accuracy_score(train_labels, train_preds_binary)
        val_acc = accuracy_score(val_labels, val_preds_binary)
        
        # ê¸°ë¡ ì €ì¥
        history['train_losses'].append(train_loss)
        history['val_losses'].append(val_loss)
        history['train_aucs'].append(train_auc)
        history['val_aucs'].append(val_auc)
        if 'train_accs' not in history:
            history['train_accs'] = []
            history['val_accs'] = []
        history['train_accs'].append(train_acc)
        history['val_accs'].append(val_acc)
        
        print(f"Epoch {epoch+1:3d}: Loss(T/V): {train_loss:.3f}/{val_loss:.3f} | "
              f"AUC(T/V): {train_auc:.3f}/{val_auc:.3f} | "
              f"Acc(T/V): {train_acc:.3f}/{val_acc:.3f}")
        
        # Best model ì €ì¥ (ì²´í¬í¬ì¸íŠ¸ í¬í•¨)
        if val_auc > history['best_val_auc']:
            history['best_val_auc'] = val_auc
            history['best_epoch'] = epoch + 1
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            best_model_path = str(checkpoint_dir / 'best_methylation_tabtransformer.pth') if checkpoint_dir else 'best_methylation_tabtransformer.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_auc': val_auc,
                'history': history
            }, best_model_path)
            print(f"  âœ… Best model saved (AUC: {val_auc:.4f}) at {best_model_path}")
            
            # ëª©í‘œ AUC ë‹¬ì„± ì•Œë¦¼ (í•˜ì§€ë§Œ ê³„ì† í›ˆë ¨)
            if val_auc >= target_auc:
                print(f"  ğŸ¯ Target AUC {target_auc:.3f} achieved! Continuing training for better performance...")
            
            # Early stopping ì¹´ìš´í„° ë¦¬ì…‹
            patience_counter = 0
        else:
            # ê°œì„ ë˜ì§€ ì•ŠìŒ - patience ì¦ê°€
            patience_counter += 1
        
        # Early stopping ì²´í¬
        if patience_counter >= early_stop_patience:
            print(f"\nâ¹ï¸  Early stopping triggered after {patience_counter} epochs without improvement")
            print(f"   Best Val AUC: {history['best_val_auc']:.4f} (Epoch {history['best_epoch']})")
            break
        
        # Learning rate scheduling
        scheduler.step(val_auc)
    
    return history

def evaluate_model(model, test_loader, model_name, device='cuda', is_cox_model=True):
    """
    ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ DataLoader
        model_name: ëª¨ë¸ ì´ë¦„
        device: ë””ë°”ì´ìŠ¤
        is_cox_model: CoxTabTransformerì¸ì§€ ì—¬ë¶€
    
    Returns:
        results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    model.eval()
    test_preds = []
    test_labels = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f"Evaluating {model_name}"):
            if is_cox_model:
                clinical_batch, omics_batch, labels_batch = batch
                clinical_batch = clinical_batch.to(device)
                omics_batch = omics_batch.to(device)
                logits, _ = model(clinical_batch, omics_batch)
            else:
                meth_batch, labels_batch = batch
                meth_batch = meth_batch.to(device)
                logits, _, _ = model(meth_batch)
            
            test_preds.extend(torch.sigmoid(logits).cpu().numpy())
            test_labels.extend(labels_batch.numpy())
    
    test_preds = np.array(test_preds).squeeze()
    test_labels = np.array(test_labels).squeeze()
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    test_auc = roc_auc_score(test_labels, test_preds)
    test_preds_binary = (test_preds > 0.5).astype(int)
    test_acc = accuracy_score(test_labels, test_preds_binary)
    
    # Confusion Matrix
    cm = confusion_matrix(test_labels, test_preds_binary)
    
    results = {
        'auc': test_auc,
        'accuracy': test_acc,
        'confusion_matrix': cm,
        'predictions': test_preds,
        'true_labels': test_labels
    }
    
    print(f"\n=== {model_name} Test Results ===")
    print(f"Test AUC: {test_auc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")
    print(f"Confusion Matrix:")
    print(f"TN: {cm[0,0]}, FP: {cm[0,1]}")
    print(f"FN: {cm[1,0]}, TP: {cm[1,1]}")
    
    return results

def main():
    """
    ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜
    """
    args = parse_arguments()
    
    # GPU ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(args.results_dir)
    results_dir.mkdir(exist_ok=True, parents=True)
    
    if args.model == 'cox':
        print("=== CoxTabTransformer í›ˆë ¨ ===")
        
        # ë°ì´í„° ë¡œë“œ
        cox_data = pd.read_parquet(Path(args.data_dir) / 'integrated_table_cox.parquet')
        clinical_data = pd.read_parquet(Path(args.data_dir) / 'processed_clinical_data.parquet')
        
        # Cox ê³„ìˆ˜ ë¡œë“œ
        _, cox_coefficients = load_cox_coefficients_by_omics(args.data_dir)
        
        # ìƒì¡´ ë¼ë²¨ ìƒì„±
        survival_labels, valid_patient_ids = create_survival_labels(clinical_data, 1095)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        cox_data_filtered = cox_data.loc[cox_data.index.intersection(valid_patient_ids)]
        clinical_data_filtered = clinical_data.loc[clinical_data.index.intersection(valid_patient_ids)]
        
        cox_continuous, cox_feature_names = prepare_cox_data(cox_data_filtered, cox_coefficients)
        clinical_categorical, vocab_sizes, encoders, clinical_feature_names = prepare_clinical_data(clinical_data_filtered)
        
        # ë¼ë²¨ ì •ë ¬
        common_patients = cox_data_filtered.index.tolist()
        labels_dict = dict(zip(valid_patient_ids, survival_labels))
        labels_aligned = np.array([labels_dict[pid] for pid in common_patients])
        
        # ë°ì´í„° ë¶„í•  (ì•™ìƒë¸”ì„ ìœ„í•´ seed ê¸°ë°˜)
        combined_data = torch.cat([clinical_categorical, cox_continuous], dim=1)
        data_split_seed = getattr(args, 'seed', 42)  # ëª…ë ¹í–‰ì—ì„œ seed ë°›ê¸°
        X_train, X_val, X_test, y_train, y_val, y_test = split_data_stratified(
            combined_data, labels_aligned, test_size=0.15, val_size=0.15, random_state=data_split_seed
        )
        
        # Clinicalê³¼ Omics ë¶€ë¶„ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
        clinical_dim = clinical_categorical.shape[1]
        X_train_clinical = X_train[:, :clinical_dim]
        X_train_omics = X_train[:, clinical_dim:]
        X_val_clinical = X_val[:, :clinical_dim]
        X_val_omics = X_val[:, clinical_dim:]
        X_test_clinical = X_test[:, :clinical_dim]
        X_test_omics = X_test[:, clinical_dim:]
        
        # DataLoader ìƒì„±
        train_dataset = TensorDataset(X_train_clinical.long(), X_train_omics, torch.tensor(y_train, dtype=torch.float32))
        val_dataset = TensorDataset(X_val_clinical.long(), X_val_omics, torch.tensor(y_val, dtype=torch.float32))
        test_dataset = TensorDataset(X_test_clinical.long(), X_test_omics, torch.tensor(y_test, dtype=torch.float32))
        
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        model = CoxTabTransformer(
            clinical_categories=vocab_sizes,
            num_omics_features=len(cox_feature_names),
            dim=64,
            depth=6,
            heads=8
        )
        
        print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
        
        # í›ˆë ¨
        history = train_cox_tabtransformer(model, train_loader, val_loader, args.epochs, args.lr, device)
        
        # í‰ê°€ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ)
        best_model_path = 'best_cox_tabtransformer.pth'
        if Path(best_model_path).exists():
            checkpoint = torch.load(best_model_path, map_location=device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… Best model loaded from {best_model_path} (AUC: {checkpoint.get('best_val_auc', 'Unknown')})")
            else:
                model.load_state_dict(checkpoint)  # êµ¬ë²„ì „ í˜¸í™˜ì„±
        else:
            print("âš ï¸ No best model found, using current model for evaluation")
        results = evaluate_model(model, test_loader, 'CoxTabTransformer', device, is_cox_model=True)
        
        # ê²°ê³¼ ì €ì¥
        final_results = {
            'model': 'CoxTabTransformer',
            'history': history,
            'test_results': {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in results.items()},
            'model_config': {
                'clinical_categories': vocab_sizes,
                'num_omics_features': len(cox_feature_names),
                'dim': 64,
                'depth': 6,
                'heads': 8
            }
        }
        
        with open(results_dir / 'cox_tabtransformer_results.json', 'w') as f:
            json.dump(final_results, f, indent=2)
        
        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir / 'cox_tabtransformer_results.json'}")
        
    elif args.model == 'methylation':
        print("=== MethylationTabTransformer í›ˆë ¨ ===")
        
        # ë©”í‹¸ë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ
        methylation_data = pd.read_parquet(Path(args.data_dir) / 'methylation_table.parquet')
        clinical_data_meth = pd.read_parquet(Path(args.data_dir) / 'processed_clinical_data_for_methylation.parquet')
        
        # ìƒì¡´ ë¼ë²¨ ìƒì„±
        survival_labels, valid_patient_ids = create_survival_labels(clinical_data_meth, 1095)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        methylation_filtered = methylation_data.loc[methylation_data.index.intersection(valid_patient_ids)]
        methylation_tensor, selected_probe_names = prepare_methylation_data(methylation_filtered, variance_threshold=0.01)
        
        # ë¼ë²¨ ì •ë ¬
        common_patients = methylation_filtered.index.tolist()
        labels_dict = dict(zip(valid_patient_ids, survival_labels))
        labels_aligned = np.array([labels_dict[pid] for pid in common_patients])
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_val, X_test, y_train, y_val, y_test = split_data_stratified(
            methylation_tensor, labels_aligned, test_size=0.15, val_size=0.15, random_state=42
        )
        
        # DataLoader ìƒì„± (ì‘ì€ ë°°ì¹˜ í¬ê¸°)
        train_dataset = TensorDataset(X_train, torch.tensor(y_train, dtype=torch.float32))
        val_dataset = TensorDataset(X_val, torch.tensor(y_val, dtype=torch.float32))
        test_dataset = TensorDataset(X_test, torch.tensor(y_test, dtype=torch.float32))
        
        train_loader = DataLoader(train_dataset, batch_size=min(args.batch_size, 16), shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=min(args.batch_size, 16), shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=min(args.batch_size, 16), shuffle=False)
        
        # ëª¨ë¸ ìƒì„±
        model = MethylationTabTransformer(
            num_probes=methylation_tensor.shape[1],
            selected_probes=min(5000, methylation_tensor.shape[1] // 10),
            dim=64,
            depth=4,
            heads=8
        )
        
        print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
        
        # í›ˆë ¨
        history = train_methylation_tabtransformer(model, train_loader, val_loader, args.epochs, args.lr, device)
        
        # í‰ê°€ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ)
        best_model_path = 'best_methylation_tabtransformer.pth'
        if Path(best_model_path).exists():
            checkpoint = torch.load(best_model_path, map_location=device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… Best model loaded from {best_model_path} (AUC: {checkpoint.get('best_val_auc', 'Unknown')})")
            else:
                model.load_state_dict(checkpoint)  # êµ¬ë²„ì „ í˜¸í™˜ì„±
        else:
            print("âš ï¸ No best model found, using current model for evaluation")
        results = evaluate_model(model, test_loader, 'MethylationTabTransformer', device, is_cox_model=False)
        
        # ê²°ê³¼ ì €ì¥
        final_results = {
            'model': 'MethylationTabTransformer',
            'history': history,
            'test_results': {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in results.items()},
            'model_config': {
                'num_probes': methylation_tensor.shape[1],
                'selected_probes': min(5000, methylation_tensor.shape[1] // 10),
                'dim': 64,
                'depth': 4,
                'heads': 8
            }
        }
        
        with open(results_dir / 'methylation_tabtransformer_results.json', 'w') as f:
            json.dump(final_results, f, indent=2)
        
        print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir / 'methylation_tabtransformer_results.json'}")

def train_ensemble_models(args):
    """ì•™ìƒë¸” ëª¨ë“œë¡œ ì—¬ëŸ¬ seedë¡œ ëª¨ë¸ í›ˆë ¨"""
    
    print(f"\nğŸ¯ Starting ensemble training with {args.n_seeds} different seeds")
    print(f"Base seed: {args.seed}, Model type: {args.model}")
    
    results_dir = Path(args.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = Path(args.checkpoint_dir) 
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    ensemble_results = []
    
    for i in range(args.n_seeds):
        current_seed = args.seed + i
        print(f"\n{'='*60}")
        print(f"ğŸŒ± Training model {i+1}/{args.n_seeds} with seed {current_seed}")
        print(f"{'='*60}")
        
        # seedë¥¼ í˜„ì¬ seedë¡œ ë³€ê²½
        args.seed = current_seed
        
        # ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰
        try:
            if args.model == 'cox':
                history = main_cox_training(args, checkpoint_dir / f'seed_{current_seed}')
            elif args.model == 'methylation':
                history = main_methylation_training(args, checkpoint_dir / f'seed_{current_seed}')
            
            # test_results JSON ì§ë ¬í™” ì²˜ë¦¬
            test_results = history.get('test_results', {})
            test_results_serializable = {}
            for key, value in test_results.items():
                if hasattr(value, 'tolist'):  # numpy arrayì¸ ê²½ìš°
                    test_results_serializable[key] = value.tolist()
                else:
                    test_results_serializable[key] = value
            
            ensemble_results.append({
                'seed': current_seed,
                'best_val_auc': history['best_val_auc'],
                'best_epoch': history['best_epoch'],
                'final_train_auc': history['train_aucs'][-1] if history['train_aucs'] else 0,
                'final_val_auc': history['val_aucs'][-1] if history['val_aucs'] else 0,
                'test_auc': history.get('test_auc', 0.0),  # Test AUC ì¶”ê°€
                'test_results': test_results_serializable  # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœ
            })
            
            print(f"âœ… Seed {current_seed} completed: Best Val AUC = {history['best_val_auc']:.4f}, Test AUC = {history.get('test_auc', 0.0):.4f}")
            
        except Exception as e:
            print(f"âŒ Seed {current_seed} failed: {str(e)}")
            ensemble_results.append({
                'seed': current_seed,
                'best_val_auc': 0.0,
                'best_epoch': 0,
                'final_train_auc': 0.0,
                'final_val_auc': 0.0,
                'test_auc': 0.0,
                'test_results': {},
                'error': str(e)
            })
    
    # ì•™ìƒë¸” ê²°ê³¼ ìš”ì•½
    print_ensemble_summary(ensemble_results, args.model, results_dir)
    
    return ensemble_results

def print_ensemble_summary(results, model_type, results_dir):
    """ì•™ìƒë¸” í›ˆë ¨ ê²°ê³¼ ìš”ì•½"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ ENSEMBLE TRAINING SUMMARY - {model_type.upper()} MODEL")
    print(f"{'='*70}")
    
    successful_results = [r for r in results if 'error' not in r]
    
    if successful_results:
        val_aucs = [r['best_val_auc'] for r in successful_results]
        test_aucs = [r.get('test_auc', 0.0) for r in successful_results]
        
        print(f"{'Seed':<8} {'Best Val AUC':<13} {'Test AUC':<10} {'Best Epoch':<10}")
        print(f"{'-'*60}")
        
        for r in successful_results:
            print(f"{r['seed']:<8} {r['best_val_auc']:<13.4f} {r.get('test_auc', 0.0):<10.4f} {r['best_epoch']:<10}")
        
        print(f"{'-'*60}")
        print(f"{'Mean':<8} {np.mean(val_aucs):<13.4f} {np.mean(test_aucs):<10.4f}")
        print(f"{'Std':<8} {np.std(val_aucs):<13.4f} {np.std(test_aucs):<10.4f}")
        print(f"{'Max':<8} {np.max(val_aucs):<13.4f} {np.max(test_aucs):<10.4f}")
        print(f"{'Min':<8} {np.min(val_aucs):<13.4f} {np.min(test_aucs):<10.4f}")
        
        # ëª©í‘œ ë‹¬ì„± ëª¨ë¸ ìˆ˜ (Test AUC ê¸°ì¤€)
        val_target_achieved = sum(1 for auc in val_aucs if auc >= 0.85)
        test_target_achieved = sum(1 for auc in test_aucs if auc >= 0.85)
        print(f"\nğŸ¯ Models achieving Val AUC â‰¥ 0.85: {val_target_achieved}/{len(successful_results)}")
        print(f"ğŸ¯ Models achieving Test AUC â‰¥ 0.85: {test_target_achieved}/{len(successful_results)}")
    
    # ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤
    failed_results = [r for r in results if 'error' in r]
    if failed_results:
        print(f"\nâŒ Failed seeds: {[r['seed'] for r in failed_results]}")
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    summary = {
        'model_type': model_type,
        'total_models': len(results),
        'successful_models': len(successful_results),
        'failed_models': len(failed_results),
        'results': results,
        'statistics': {
            'val_auc_mean': float(np.mean([r['best_val_auc'] for r in successful_results])) if successful_results else 0,
            'val_auc_std': float(np.std([r['best_val_auc'] for r in successful_results])) if successful_results else 0,
            'test_auc_mean': float(np.mean([r.get('test_auc', 0.0) for r in successful_results])) if successful_results else 0,
            'test_auc_std': float(np.std([r.get('test_auc', 0.0) for r in successful_results])) if successful_results else 0,
            'val_target_achieved_count': sum(1 for r in successful_results if r['best_val_auc'] >= 0.85),
            'test_target_achieved_count': sum(1 for r in successful_results if r.get('test_auc', 0.0) >= 0.85)
        }
    }
    
    # íŒŒì¼ëª…ì„ ë” ëª…í™•í•˜ê²Œ: cox_tabtransformer_ensemble_results.json
    summary_path = results_dir / f'{model_type}_tabtransformer_ensemble_results.json'
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nğŸ“Š Ensemble summary saved: {summary_path}")
    print(f"{'='*70}")

def main_cox_training(args, checkpoint_dir=None):
    """Cox ëª¨ë¸ ë‹¨ì¼ í›ˆë ¨ (ì•™ìƒë¸”ì—ì„œ í˜¸ì¶œ)"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    print(f"Training with seed: {args.seed}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(args.results_dir)
    results_dir.mkdir(exist_ok=True, parents=True)
    
    if checkpoint_dir:
        checkpoint_dir = Path(checkpoint_dir)
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
    
    print("=== CoxTabTransformer í›ˆë ¨ ===")
    
    # ë°ì´í„° ë¡œë“œ
    cox_data = pd.read_parquet(Path(args.data_dir) / 'integrated_table_cox.parquet')
    clinical_data = pd.read_parquet(Path(args.data_dir) / 'processed_clinical_data.parquet')
    
    # Cox ê³„ìˆ˜ ë¡œë“œ
    _, cox_coefficients = load_cox_coefficients_by_omics(args.data_dir)
    
    # ìƒì¡´ ë¼ë²¨ ìƒì„±
    survival_labels, valid_patient_ids = create_survival_labels(clinical_data, 1095)
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    cox_data_filtered = cox_data.loc[cox_data.index.intersection(valid_patient_ids)]
    clinical_data_filtered = clinical_data.loc[clinical_data.index.intersection(valid_patient_ids)]
    
    cox_continuous, cox_feature_names = prepare_cox_data(cox_data_filtered, cox_coefficients)
    clinical_categorical, vocab_sizes, encoders, clinical_feature_names = prepare_clinical_data(clinical_data_filtered)
    
    # ë¼ë²¨ ì •ë ¬
    common_patients = cox_data_filtered.index.tolist()
    labels_dict = dict(zip(valid_patient_ids, survival_labels))
    labels_aligned = np.array([labels_dict[pid] for pid in common_patients])
    
    # ë°ì´í„° ë¶„í•  (ì•™ìƒë¸”ì„ ìœ„í•´ seed ê¸°ë°˜)
    combined_data = torch.cat([clinical_categorical, cox_continuous], dim=1)
    X_train, X_val, X_test, y_train, y_val, y_test = split_data_stratified(
        combined_data, labels_aligned, test_size=0.15, val_size=0.15, random_state=args.seed
    )
    
    # Clinicalê³¼ Omics ë¶€ë¶„ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
    clinical_dim = clinical_categorical.shape[1]
    X_train_clinical = X_train[:, :clinical_dim]
    X_train_omics = X_train[:, clinical_dim:]
    X_val_clinical = X_val[:, :clinical_dim]
    X_val_omics = X_val[:, clinical_dim:]
    X_test_clinical = X_test[:, :clinical_dim]
    X_test_omics = X_test[:, clinical_dim:]
    
    # DataLoader ìƒì„±
    train_dataset = TensorDataset(X_train_clinical.long(), X_train_omics, torch.tensor(y_train, dtype=torch.float32))
    val_dataset = TensorDataset(X_val_clinical.long(), X_val_omics, torch.tensor(y_val, dtype=torch.float32))
    test_dataset = TensorDataset(X_test_clinical.long(), X_test_omics, torch.tensor(y_test, dtype=torch.float32))
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # ëª¨ë¸ ìƒì„± (ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•´ ë³µì¡ë„ ê°ì†Œ)
    model = CoxTabTransformer(
        clinical_categories=vocab_sizes,
        num_omics_features=len(cox_feature_names),
        dim=64,   # 128 â†’ 64 (ë³µì¡ë„ ê°ì†Œ)
        depth=4,  # 8 â†’ 4 (ë” ì–˜ì€ ëª¨ë¸)
        heads=8,  # 16 â†’ 8 (ì–´í…ì…˜ í—¤ë“œ ê°ì†Œ)
        attn_dropout=0.3,  # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ
        ff_dropout=0.3     # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ
    )
    
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # í›ˆë ¨
    history = train_cox_tabtransformer(
        model, train_loader, val_loader, 
        epochs=args.epochs, 
        lr=args.lr, 
        device=device,
        checkpoint_dir=checkpoint_dir,
        resume_from=args.resume_from if hasattr(args, 'resume_from') else None,
        target_auc=args.target_auc if hasattr(args, 'target_auc') else 0.85
    )
    
    # í‰ê°€ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ)
    best_model_name = f'best_cox_seed_{args.seed}.pth' if checkpoint_dir else 'best_cox_tabtransformer.pth'
    best_model_path = checkpoint_dir / best_model_name if checkpoint_dir else best_model_name
    
    if Path(best_model_path).exists():
        checkpoint = torch.load(best_model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… Best model loaded from {best_model_path}")
    
    results = evaluate_model(model, test_loader, 'CoxTabTransformer', device, is_cox_model=True)
    
    # ê²°ê³¼ì— test AUC ì¶”ê°€
    history['test_auc'] = results.get('auc', 0.0)
    history['test_results'] = results
    
    return history

def main_methylation_training(args, checkpoint_dir=None):
    """Methylation ëª¨ë¸ ë‹¨ì¼ í›ˆë ¨ (ì•™ìƒë¸”ì—ì„œ í˜¸ì¶œ)"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    print(f"Training with seed: {args.seed}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(args.results_dir)
    results_dir.mkdir(exist_ok=True, parents=True)
    
    if checkpoint_dir:
        checkpoint_dir = Path(checkpoint_dir)
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
    
    print("=== MethylationTabTransformer í›ˆë ¨ ===")
    
    # ë°ì´í„° ë¡œë“œ
    methylation_data = pd.read_parquet(Path(args.data_dir) / 'methylation_data_for_tabtransformer.parquet')
    clinical_data = pd.read_parquet(Path(args.data_dir) / 'processed_clinical_data.parquet')
    
    # Methylationê³¼ clinical ë°ì´í„°ì˜ ê³µí†µ í™˜ì ì°¾ê¸°
    common_patients_meth = methylation_data.index.intersection(clinical_data.index)
    methylation_data_filtered = methylation_data.loc[common_patients_meth]
    clinical_data_meth = clinical_data.loc[common_patients_meth]
    
    # ìƒì¡´ ë¼ë²¨ ìƒì„±
    survival_labels, valid_patient_ids = create_survival_labels(clinical_data_meth, 1095)
    
    # ìœ íš¨í•œ í™˜ìë“¤ë¡œ í•„í„°ë§
    methylation_data_filtered = methylation_data_filtered.loc[
        methylation_data_filtered.index.intersection(valid_patient_ids)
    ]
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    methylation_tensor, selected_probes = prepare_methylation_data(methylation_data_filtered, variance_threshold=0.01)
    
    # ë¼ë²¨ ì •ë ¬
    common_patients_final = methylation_data_filtered.index.tolist()
    labels_dict = dict(zip(valid_patient_ids, survival_labels))
    labels_aligned = np.array([labels_dict[pid] for pid in common_patients_final])
    
    # ë°ì´í„° ë¶„í•  (seed ê¸°ë°˜)
    X_train, X_val, X_test, y_train, y_val, y_test = split_data_stratified(
        methylation_tensor, labels_aligned, test_size=0.15, val_size=0.15, random_state=args.seed
    )
    
    # DataLoader ìƒì„±
    train_dataset = TensorDataset(X_train, torch.tensor(y_train, dtype=torch.float32))
    val_dataset = TensorDataset(X_val, torch.tensor(y_val, dtype=torch.float32))
    test_dataset = TensorDataset(X_test, torch.tensor(y_test, dtype=torch.float32))
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
    
    # ëª¨ë¸ ìƒì„± (ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•´ ë³µì¡ë„ ê°ì†Œ)
    model = MethylationTabTransformer(
        num_probes=methylation_tensor.shape[1],
        selected_probes=min(5000, methylation_tensor.shape[1] // 10),  # í”„ë¡œë¸Œ ìˆ˜ ê°ì†Œ
        dim=64,   # 128 â†’ 64 (ë³µì¡ë„ ê°ì†Œ)
        depth=4,  # 6 â†’ 4 (ë” ì–•ì€ ëª¨ë¸)
        heads=8,  # 16 â†’ 8 (ì–´í…ì…˜ í—¤ë“œ ê°ì†Œ)
        attn_dropout=0.3,  # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ
        ff_dropout=0.3     # ë” ê°•í•œ ë“œë¡­ì•„ì›ƒ
    )
    
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # í›ˆë ¨
    history = train_methylation_tabtransformer(
        model, train_loader, val_loader, 
        epochs=args.epochs, 
        lr=args.lr, 
        device=device,
        checkpoint_dir=checkpoint_dir,
        resume_from=args.resume_from if hasattr(args, 'resume_from') else None,
        target_auc=args.target_auc if hasattr(args, 'target_auc') else 0.85
    )
    
    # í‰ê°€ (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ)
    best_model_name = f'best_methylation_seed_{args.seed}.pth' if checkpoint_dir else 'best_methylation_tabtransformer.pth'
    best_model_path = checkpoint_dir / best_model_name if checkpoint_dir else best_model_name
    
    if Path(best_model_path).exists():
        checkpoint = torch.load(best_model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… Best model loaded from {best_model_path}")
    
    results = evaluate_model(model, test_loader, 'MethylationTabTransformer', device, is_cox_model=False)
    
    # ê²°ê³¼ì— test AUC ì¶”ê°€
    history['test_auc'] = results.get('auc', 0.0)
    history['test_results'] = results
    
    return history

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='TabTransformer í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--model', choices=['cox', 'methylation'], required=True, help='ëª¨ë¸ íƒ€ì…')
    parser.add_argument('--epochs', type=int, default=50, help='í›ˆë ¨ ì—í­ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=1e-4, help='í•™ìŠµë¥ ')
    parser.add_argument('--data_dir', type=str, default='../data/processed', help='ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--results_dir', type=str, default='../results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--seed', type=int, default=42, help='ëœë¤ ì‹œë“œ (ì•™ìƒë¸”ìš©)')
    parser.add_argument('--ensemble', action='store_true', help='ì•™ìƒë¸” ëª¨ë“œ í™œì„±í™”')
    parser.add_argument('--n_seeds', type=int, default=5, help='ì•™ìƒë¸”ì— ì‚¬ìš©í•  ì‹œë“œ ê°œìˆ˜')
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--resume_from', type=str, default=None, help='ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--target_auc', type=float, default=0.85, help='ëª©í‘œ AUC ì ìˆ˜')
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    
    if args.ensemble:
        train_ensemble_models(args)
    else:
        main()